\documentclass[12pt]{report} %fuente a 12pt

% MÁRGENES: 2,5 cm sup. e inf.; 3 cm izdo. y dcho.
\usepackage[
a4paper,
vmargin=2.5cm,
hmargin=3cm
]{geometry}

% ENLACES
\usepackage{hyperref}
\hypersetup{colorlinks=true,
	linkcolor=black, % enlaces a partes del documento (p.e. índice) en color negro
    urlcolor=blue} % enlaces a recursos fuera del documento en azul
    
\usepackage[spanish, es-tabla]{babel}
\usepackage{graphicx}
\usepackage{float}
\usepackage{listings}
\usepackage[titletoc,toc,page]{appendix}
\usepackage{adjustbox}
\usepackage{rotating}
\usepackage{caption}
\renewcommand{\appendixtocname}{Ap\'endices}
\renewcommand{\appendixpagename}{Ap\'endices}

% REVISAR FUENTES
% REVISAR FOTOS
% REVISAR NEGRITAS
% REVISAR CURSIVA

\title{Lyncex: describiendo una aplicación web como conocimiento}
\author{Adrián Arroyo Calle}
\date{Curso 2019-2020}

\begin{document}

\maketitle

\tableofcontents

\listoffigures

\listoftables

\chapter{Introducción}

\section{Motivación}

En ciencias de la computación, de forma recurrente se divide entre código, lo que va a ejecutar la máquina, y datos.
Esta diferencia, aunque pueda resultar evidente, es innecesaria, ya que el código no deja de ser dato, solo que con una semántica diferente.
Von Neuman, en su modelo de computadora, elimina las diferencias a nivel de hardware entre código y datos, de modo muy exitoso, hasta tal punto que esta idea sigue siendo la base de los procesadores modernos actuales.
Hoy día en la creación de aplicaciones web, separamos por un lado el código y por otro los datos que van a circular a través de él. 
No obstante, considero interesante imaginar y plantear una aplicación web descrita de la misma forma en que se describen los datos.
De forma principalmente declarativa y usando tecnologías maduras como RDF como la base del modelo de datos.
El servidor web pasa a ser una base de datos, dónde las diferencias entre código y datos son puramente semánticas.

De este modo, podríamos resumir este sistema como una base de datos y a la vez un servidor web configurable a través del contenido semántico de la propia base de datos.

La idea surge principalmente de estudiar la base de datos CouchDB (se detallará más adelante) y su concepto de integrar código JavaScript como un tipo de documento más.

\section{Objetivos}

El objetivo principal de este trabajo es construir una entorno de desarrollo que implemente el concepto de base de datos y servidor web en un mismo entorno, usando tripletas para su representación. Sobre esta plataforma deberán poder ejecutarse aplicaciones simples y la llamaremos a partir de ahora, Lyncex.

La implementación resultante no deberá ser de un nivel de producción, pero deberá ser útil para validar los conceptos con los que se va a trabajar, así como el estado de ciertas tecnologías con de relativa poca popularidad en el mundo empresarial.

Además de la implementación en código, se definirá una ontología que define el funcionamiento de la aplicación web. Esta ontología es una interfaz sobre la que un trabajo posterior podría diseñar implementaciones alternativas, mejorando el código original o extendiendo la funcionalidad.

En este proyecto se reforzarán los conocimientos del lenguaje Prolog, en concreto en una vertiente altamente práctica, como es su utilidad para desarrollo de aplicaciones web.

\section{Alcance}

Para cumplir los objetivos identificamos cinco historias de usuario diferentes. En este proyecto intentaremos completar las cinco historias de usuario al completo. Cada historia de usuario refleja un conjunto de funcionalidades que debemos tener implementadas para dar por completada la historia de usuario. Más adelante entraremos en detalle de los requisitos que llevan aparejados cada una de estas historias de usuario.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{plantuml/dependency.png}
    \caption{Dependencias entre las historias de usuario}
    \label{fig:dependencia}
\end{figure}

De estas historias, cuatro de ellas son un incremento funcional del mismo sistema, Lyncex, y se debe completar una antes de pasar a la siguiente. La quinta, el desarrollo de la ontología, se desarrolla de forma paralela al resto del sistema.

El proyecto deberá estar finalizado antes de la fecha límite, del 10 de julio de 2020, tanto la implementación como esta memoria.

El proyecto no trata de crear ninguna base de datos desde cero, ni enfrentarse a muchos de los retos comunes que existen en este tipo de sistemas como restricciones de rendimiento o problemas de escalabilidad y sincronización. Tampoco intenta crear un framework web listo para ir a producción, ya que siguen faltando componentes esenciales de cualquier framework web.

\section{Aplicación de ejemplo (visión general)}

De nada serviría una plataforma de desarrollo completa sin aplicaciones que la utilicen. Para demostrar la funcionalidad del sistema, en entornos reales, se diseñará una aplicación basada en datos abiertos ya existentes.

Se usarán los datos de Bibliotecas de Castilla y León, en formato RDF/XML y se tratará de hacer un ejemplo práctico de aplicación modelo que usa Lyncex.

\section{Estructura de la memoria}
La memoria se estructura en diferentes capítulos. En primer lugar, se repasarán conocimientos previos necesarios para entender, desde una base técnica pero no especializada, el conjunto de este trabajo. Posteriormente se realizará el análisis de diferentes tecnologías similares, que cubren parte de los objetivos de este trabajo de alguna u otra forma, el conocido como estado del arte.

Posteriormente se tratará la planificación del proyecto, haciendo un inciso en la metodología Scrum. Posteriormente se pasará a la etapa de análisis, donde se descompondrán los requisitos del proyecto y las historias de usuario a realizar.

El siguiente capítulo está dedicado al diseño, tanto del propio software, como de la ontología que lo acompaña. A continuación, se encuentra el capítulo de la implementación. En este capítulo primero veremos herramientas que se han usado para la construcción de este proyecto. Después veremos la implementación del software en sí, haciendo especial hincapié en aquellos elementos que no sean del todo evidentes para el lector.

El capítulo dedicado a la validación y las pruebas se ubica inmediatamente después. En este capítulo se describirán las pruebas realizadas y la metodología empleada para comprobar que efectivamente, se está implementando correctamente el software.

Le sigue el capítulo dedicado a la aplicación de ejemplo, donde se llevará a cabo un mini proyecto (objetivo, análisis, diseño, implementación y pruebas) para ver que el software como plataforma de desarrollo para otras aplicaciones tiene sentido o no, y poder identificar posibles inconvenientes prácticos que se hayan encontrado.

A modo de referencia, el capítulo de manuales, incluye instrucciones de instalación y de uso de Lyncex.

Por último, se hablará sobre las conclusiones que podemos extraer del proyecto así como trabajo que podría realizarse en el futuro siguiendo por este camino.


\chapter{Conocimientos previos}

En este capítulo trataremos de explicar de la forma más clara posible, conocimientos que creemos necesarios para entender el proyecto en su totalidad.

\section{Web Semántica}

La web semántica es un concepto amplio, surgido a finales de los 90 y en un principio impulsado por Tim Berners-Lee y su organización, el W3C. La idea principal es dar un paso cualtitativo más en el acceso y descripción de datos respecto al modelo de la web HTML. Dentro de este paraguas existen numerosos proyectos como RDF, SPARQL, LinkedData, ... En este proyecto solo utilizamos algunas de ellas.

\subsection{RDF}

RDF es una de las tecnologías base, sobre las que se asienta prácticamente toda la web semántica. Se trata de un modelo de representación de la información basado en tripletas.

\subsubsection{Tripletas}
Las tripletas son un modo de representar la información mediante tríos, ordenados, de datos.
Es uno de los modos fundamentales de representación de información

En una tripleta, los elementos se denominan, en este orden, sujeto, predicado y objeto. La estructura básica sigue la de la gramática del lenguaje humano para afirmar hechos.
El sujeto es el ente sobre el que vamos a afirmar algo, el predicado es la acción y/o propiedad del sujeto y el objeto, que es el contenido de la propiedad que se define.
Veamos algunos ejemplos de este concepto:
\begin{verbatim}
maria -> likes -> chocolate
maria -> is -> human
adrian -> likes -> maria
\end{verbatim}
Con estos tres elementos, y suponiendo que podemos combinar sujetos, predicados y objetos en otras tripletas, podemos crear grandes redes interrelacionadas, ideales para el almacenamiento de conocimiento.
Estas se denominan grafos.
Tres es el número mínimo necesario para poder modelar cualquier descriptible. Las duplas (dos elementos) también son populares en bases de datos, pero necesitan de un contexto externo para su interpretación correcta.
Este sistema además tiene varias ventajas, por ejemplo, la duplicación de las tripletas no es un problema, ya que simplemente se reafirma lo mismo una y otra vez.

La idea de las tripletas es bastante genérica y existen diferentes implementaciones de la idea. La más popular, sin lugar a dudas, es RDF, que describiremos a continuación.
Las tripletas tienen numerosas ventajas pero también diversos inconvenientes, entre ellos: rendimiento inferior y mayor dificultad a la hora de actualizar el contenido de las tripletas.
Esto, sumado a la falta de educación en este concepto, ha provocado que actualmente, el uso de tripletas sea minoritario.

\subsubsection{Tripletas en RDF}

RDF (Resource Description Framework) es un estándar desarrollado por Tim Berners-Lee en los años 90.
RDF define un modelo concreto para usar tripletas derivado de la World Wide Web, para ello se impone que los elementos o términos puedan ser solo de tres tipos: IRIs, blank nodes y literales.
Las IRIs son la versión internacionalizada y ampliada de las famosas URL. Cada IRI representa un recurso, tratándose de un identificadores universal y son la base de las interconexiones entre tripletas. En RDF se pueden usar IRIs en el sujeto, el predicado y el objeto.
Los literales son datos puros, sin conexión entre sí. Pueden ser cadenas de texto, números enteros, números decimales, etc... Solo los objetos pueden ser literales.
Los blank nodes son conexiones anónimas, sin definir explícitamente, entre varias tripletas. Solo pueden ser blank nodes los sujetos y los objetos.

Además RDF define ciertos recursos de utilidad como un tipado y una distinción entre varios tipos elementales de recurso.

RDF se define sin representación textual por defecto, aunque existen varios estándares. Uno de los originales es RDF-XML, que usa la sintaxis de XML para definir RDF. Esta sintaxis es especialmente compleja, ya que es verbosa y admite varios modos de uso.
Otros estándares definen JSON-LD que usa sintaxis de JSON, otro formato genérico popular o RDFa que se integra en HTML. Sin embargo, también existen sintaxis expresamente diseñadas para RDF, que por lo general son más claras que tratar de adaptar otro formato a RDF.
Algunos ejemplos de estas sintaxis son Turtle, N3, NTriples y TriG.

En Lyncex se ha decidido usar sintaxis Turtle. Un ejemplo de esta sintaxis a continuación:

\begin{lstlisting}
@base <https://lyncex.com/lyncex#> .
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix foaf: <http://xmlns.com/foaf/0.1/> .

<Richard_Feynman>
    a foaf:Person ;
    foaf:name "Richard Feynman" ;
    foaf:birthday "02-15" ;
    foaf:knows [
        a foaf:Person ;
        foaf:name "Albert Einstein"
    ] ;
    foaf:knows <Von_Neumman> .

<Von_Neumman>
    a foaf:Person ;
    foaf:name "John Von Neumman" .
\end{lstlisting}

Que es equivalente a:

\begin{adjustbox}{width=\textwidth,keepaspectratio}
\begin{lstlisting}
https://lyncex.com/lyncex#Richard_Feynman,http://www.w3.org/1999/02/22-rdf-syntax-ns#type,http://xmlns.com/foaf/0.1/Person
https://lyncex.com/lyncex#Richard_Feynman,http://xmlns.com/foaf/0.1/name,Richard Feynman
https://lyncex.com/lyncex#Richard_Feynman,http://xmlns.com/foaf/0.1/birthday,"02-15"
https://lyncex.com/lyncex#Richard_Feynman,http://xmlns.com/foaf/0.1/knows,_:1
https://lyncex.com/lyncex#Richard_Feynman,http://xmlns.com/foaf/0.1/knows,https://lyncex.com/lyncex#Von_Neumman
_:1,http://www.w3.org/1999/02/22-rdf-syntax-ns#type,http://xmlns.com/foaf/0.1/Person
_:1,http://xmlns.com/foaf/0.1/name,Albert Einstein
https://lyncex.com/lyncex#Von_Neumman,http://www.w3.org/1999/02/22-rdf-syntax-ns#type,http://xmlns.com/foaf/0.1/Person
https://lyncex.com/lyncex#Von_Neumman,http://xmlns.com/foaf/0.1/name,John Von Neumman
\end{lstlisting}
\end{adjustbox}

En Turtle primero se inicia definiendo una base para las IRIs y prefijos para no tener que escribir IRIs largas y poco legibles en el resto del documento.
Las IRIs se definen usando los símbolos < y >. Las IRIs con prefijo llevan el prefijo y el símbolo :. 
Los literales llevan comillas y los blank nodes se definen con [ y ] y en su interior se definen las tripletas teniendo como sujeto el propio blank node.

En Turtle existe una forma sencilla de no repetir el sujeto, útil para afirmar varias cosas sobre un mismo sujeto. Se utiliza el símbolo ; para iniciar una nueva tripleta sobre el mismo sujeto.

Al acabar de afirmar cosas sobre un sujeto, se pone un punto. Un azúcar sintáctico usado en el ejemplo es que la propiedad a es equivalente a \begin{verbatim}http://www.w3.org/1999/02/22-rdf-syntax-ns#type\end{verbatim}.

\subsection{RDF schema y ontologías}

RDF es un modelo de representación de la información muy potente, sin embargo, tal y como lo hemos definido, puede volverse muy caótico en poco tiempo.

La razón es que no hay ningún requisito per sé de como tienen que estructurarse los datos dentro de las tripletas. El principal inconveniente de esto es la interoperabilidad entre diferentes fuentes de información y entre diferentes programas.

Para solucionarlo, una idea que se puede tener es disponer de unos metadatos que informen de como se debe manejar esta información. Esta información que describe la estructura de otra información se denomina ontología. Ya que las ontologías no son más que información, se incorpora como tripletas también y no existe diferencia física de entre las tripletas de una ontología y la de una información cualquiera, la diferencia es puramente semántica. 

Existen numerosos lenguajes de ontologías bajo RDF, uno de los primeros, impulsado como estándar por el W3C es RDF Schema. Existen otros lenguajes más avanzados como OWL, que incluyen soporte para descripciones más elaboradas.

La base de RDF Schema es la clase, o rdfs:Class, usando la IRI con prefijos. Las clases a su vez pueden ser subclases de otra con rdfs:subClassOf. Las clases podemos usarlas para definir tipos nuevos, que luego se referencian usando la propiedad rdf:type.

A su vez, también podemos definir los predicados, también llamados propiedades. Esto ya venía en RDF, bajo el tipo rdf:Property, pero RDF Schema añade predicados extras como rdfs:domain y rdfs:range. El primero permite definir en una tripleta que usa esta propiedad los elementos de que tipo pueden ser sujetos. Y en el caso del range que clase de valores de aceptan como objeto. Las propiedades se pueden heredar también mediante rdfs:subPropertyOf.

Con estos elementos podemos crear ontologías con esquemas similares a los que tendríamos en programación orientada a objetos, aunque de forma mucho más flexible.

Veamos un ejemplo de ontología sencilla definida con RDF Schema.

\begin{lstlisting}
@base <https://lyncex.com/> .
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

<Book>
    a rdfs:Class ;
    rdfs:label "Book" ;
    rdfs:comment "A work printed on sheets of paper" .

<name>
    a rdf:Property ;
    rdfs:domain <Book> ;
    rdfs:range xsd:string .

<Comic>
    a rdfs:Class .
    rdfs:subClassOf <Book> ;
    rdfs:label "Comic" ;
    rdfs:comment "A book containing comic strips" .

\end{lstlisting}

En este ejemplo se usa además la ontología XMLSchema, que provee de tipos básicos derivados de XML (entre otras cosas).

Una de las ideas básicas de la web semántica es que las ontologías puedan ser reutilizables por distintas partes. Para describir cierta información muy común existen ontologías establecidas tanto de forma general como para nichos específicos (bioinformática, geografía). Algunos de los hogares de ontologías más importantes son la propia W3C, la iniciativa Dublin Core y Schema.org, iniciativa esta última de los grandes buscadores (Google, Bing, Yandex). Estas ontologías pueden ir versionadas.


\subsection{SPARQL}

Otro de los estándares definidos por W3C bajo el paraguas de la web semántica es SPARQL. Se trata de un lenguaje de consulta sobre RDF. Además existe una extensión del estándar SPARQL Update, que permite modificar el contenido de la base de datos. Sin embargo, a pesar de llamarse "Update", no existe ninguna operación para esto. Solo existe borrado y creación de nuevas tripletas. Esto es un problema que en Lyncex hemos visto también y que tiene que ver con las bases de RDF y su dificultad de actualización.

Una consulta SPARQL empieza opcionalmente con prefijos, muy similares a los de Turtle y con el mismo objetivo. Posteriormente le sigue un SELECT, que indica que variables se deben sacar como resultado. La salida será un listado de una tupla de los valores que hayamos indicado. Si quisiésemos añadir operaciones de agregación, las indicaríamos aquí también.

Las variables empiezan siempre por interrogación y adoptan los valores que cumplen con todas las afirmaciones del WHERE. Las afirmaciones del WHERE, son tripletas que deben existir todas ellas en la base de datos (es decir, es un AND implícito). Para combinar tripletas entre sí podemos usar variables intermedias o property paths, que es una sintaxis más corta. Si queremos excluir algunas tripletas de este AND podemos usar OPTIONAL, donde se intentará cumplir los criterios pero si no se cumple, será como si no existiera este bloque. Dentro de los WHERE podemos incluir FILTER, que realizan comprobaciones sobre las variables.

Veamos un ejemplo de consulta:

\begin{lstlisting}
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
PREFIX lyncex: <https://lyncex.com/>

SELECT ?name ?author
WHERE {
    ?id rdf:type lyncex:Book .
    ?id lyncex:name ?name .
    ?id lyncex:author ?author .
}
\end{lstlisting}

Un ejemplo algo más avanzado, con datos de Wikidata:

\begin{lstlisting}
PREFIX wd: <http://www.wikidata.org/entity/>
PREFIX wdt: <http://www.wikidata.org/prop/direct/>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

SELECT ?city_name ?brother_name
WHERE {
    ?city wdt:P31 wd:Q515 .
    ?city wdt:P17 wd:Q29 .
    ?city rdfs:label ?city_name .
    OPTIONAL {
        ?city wdt:P190/rdfs:label ?brother_name .
        FILTER (lang(?brother_name) = 'es') .
    }
    FILTER (lang(?city_name) = 'es')
}
\end{lstlisting}

Existen más modos de consulta además de SELECT. En particular ASK, que simplemente verifica si se cumplen las condiciones de un WHERE en la base de datos, y CONSTRUCT, que permite sacar por salida tripletas, en vez de un listado de tuplas.

\section{Prolog}

Prolog es un lenguaje de programación lógico diseñado por Alain Colmerauer, Philippe Roussel y Robert Kowalski en 1972 en Marsella, Francia. Se trata de uno de los primeros y más populares lenguajes de programación lógica. Este se basa en la lógica de primer orden con una base de conocimiento compuesta de hechos y reglas. 

El flujo de ejecución de Prolog es una búsqueda con backtracking. Ante un predicado, Prolog trata de demostrar su veracidad, identificando valores para variables si es necesario. En caso de encontrarse con algo indemostrable, vuelve para atrás deshaciendo todo lo hecho hasta llegar a un punto de elección, donde toma otro valor para las variables o elige otra regla que satisfaga el predicado.

Para una implementación eficiente de estos conceptos, Prolog usa cláusulas Horn y un procedimiento conocido como unificación.

\subsection{Base de conocimiento}

Un programa Prolog se define por su base de conocimiento. La forma principal de construir una base de conocimiento es mediante ficheros de texto, sin embargo, esta base de conocimiento también se puede modificar en tiempo de ejecución mediante los predicados assert y retractall, es por ello que se dice que Prolog es un lenguaje homoicónico, ya que la diferencia entre código y datos es inexistente.

Un código cualquiera está compuesto de comentarios y términos. Los términos pueden ser hechos o reglas. La única diferencia es que las reglas tienen condiciones que tienen que verificarse también para poder verificar la validez del término. Con un hecho, su mera definición es suficiente. A su vez los términos de componen de átomos (simples o compuestos), que son elementos constantes y variables, que pueden adoptar un valor. Los átomos empiezan en minúscula y las variables empiezan por mayúscula.

\begin{lstlisting}[language=Prolog]

human(socrates).
human(kant).
mortal(X) :- human(X).

\end{lstlisting}

Si cargamos una base de conocimiento con ese fichero podremos ejecutar las siguientes consultas.

\begin{verbatim}
Welcome to SWI-Prolog (threaded, 64 bits, version 8.1.29)
SWI-Prolog comes with ABSOLUTELY NO WARRANTY. This is free software.
Please run ?- license. for legal details.

For online help and background, visit https://www.swi-prolog.org
For built-in help, use ?- help(Topic). or ?- apropos(Word).

?- human(socrates).
true.

?- human(pepito).
false.

?- mortal(X).
X = socrates ;
X = kant.

?- assertz(god(zeus)).
true.

?- god(X).
X = zeus.

?- retractall(god(zeus)).
true.

?- god(X).
false.

?- 
\end{verbatim}

A pesar de ser un lenguaje enfocado a consultas, estas pueden ser transparentes al usuario, ya que existen muchos tipos de predicados especiales como, leer de teclado, imprimir pantalla o incluso ejecutar un servidor HTTP.

\subsection{Operadores y aritmética}

En Prolog, muchos operadores tradicionales son ligeramente diferentes por lo que haremos un repaso. El operador lógico AND es simplemente la coma, usado frecuentemente en reglas complejas. El operador lógico OR es el punto y coma, aunque normalmente se redefine otra regla, y propiciar el funcionamiento del backtracking, ya que queda más legible. Por último, el operador NOT se define como \\+. En Prolog la negación es negación por fallo. Es decir, tratará como fallo aquello que no puede demostrar como true. Esto solo tiene sentido bajo ciertas suposiciones, algunas de las cuales RDF no tiene como la de mundo cerrado (RDF es mundo abierto).

En Prolog existen varios operadores con un funcionamiento similar a la igualdad. Por un lado el operador = realiza la operación de unificación a ambos lados (simétrico). La operación de unificación trata de que los átomos o variables que tengan a los lados coincidan. Por poner un ejemplo sencillo, si en un lado del = tenemos un átomo y en otro una variable, la forma de que coincidan es que la variable adopte el valor del átomo y siempre se verificará a no ser que la variable ya hubiese adoptado otro valor antes que no sea del del átomo. Si por ejemplo hay dos átomos, se comprueba que sean exactamente iguales.

Merece especial mención el caso de las operaciones aritméticas, ya que en Prolog los símbolos aritméticos se quedan formando parte del átomo y no se evalúan, salvo que se pida explícitamente. Por ello, podemos decir que 4+5 no unifica no 3+6, aunque sus evaluaciones matemáticas sí unificarían entre sí.

Para forzar la evaluación podemos usar el operador is, que es asimétrico y evalúa solo el lado derecho. También existe el operador =:= que evalúa de forma simétrica.

Veamos algunos ejemplos de todo esto:
\begin{lstlisting}[language=Prolog]
Welcome to SWI-Prolog (threaded, 64 bits, version 8.1.29)
SWI-Prolog comes with ABSOLUTELY NO WARRANTY. This is free software.
Please run ?- license. for legal details.

For online help and background, visit https://www.swi-prolog.org
For built-in help, use ?- help(Topic). or ?- apropos(Word).

?- write('Un término '), write('Otro término').
Un término Otro término
true.

?- write('Un término '); write('Otro término').
Un término 
true .

?- fail.
false.

?- \+ fail.
true.

?- X = Y.
X = Y.

?- socrates = X.
X = socrates.

?- socrates = platon.
false.

?- book(Name, 'Cervantes') = book('Don Quijote', Author).
Name = 'Don Quijote',
Author = 'Cervantes'.

?- 4+5=3+6.
false.

?- X is 3+6.
X = 9.

?- 4+5=:=3+6.
true.

?- 
\end{lstlisting}

\subsection{Listas y diccionarios}

Aparte de los términos, existen otros tipos de estructuras de datos como las listas y los diccionarios. En el fondo siguen siendo combinaciones de átomos y variables, pero disponen de sintaxis especial.

Las listas se definen usando corchetes, y sus elementos se separan por comas. Además, podemos incluir una barra vertical para separar el primer elemento (head) del resto (tail). Una regla de suma de una lista sería así:

\begin{lstlisting}[language=Prolog]
sum([], 0).
sum([H|T], Out) :-
    sum(T, X),
    Out is H+X.
\end{lstlisting}

\begin{lstlisting}
?- sum([1,2,3], X).
X = 6.
\end{lstlisting}

Algunos términos de interés para trabajar con listas son: member, append, length, nth1 y nth0.

Los diccionarios por otro lado son estructuras de datos clave valor. Se definen mediante llaves y se separan los pares por coma, mientras que cada elemento del par se separa por dos puntos. Los diccionarios en Prolog además pueden tener una etiqueta, aunque si no queremos, podemos usar la barra baja para aceptar todo tipo de etiqueta. Existe un acceso mediante la sintaxis punto, pero además existen diversos términos por defecto como dict\_create, dict\_pairs, get\_dict, put\_dict, dicts\_join y operadores como :< y >:<.

\begin{lstlisting}
?- X = _{world:mundo, hello:hola}, write(X.hello), write(X.world).
holamundo
X = _9158{hello:hola, world:mundo}.
\end{lstlisting}

\subsection{Metapredicados}

Uno de los detalles más importantes de Prolog son sus capacidades de metaprogramación. Además de la flexibilidad que ofrecen, permiten hacer código mucho más compacto.

El metapredicado principal es call, que permite llamar al término que deseemos. La interfaz de comandos de Prolog no deja de ser un call por debajo. La flexibilidad es que el término al que llamamos no tiene por qué ser siempre el mismo. Sin embargo, este término suele ser demasiado bajo nivel para la mayoría de casos.

Por encima tenemos forall. forall toma primero un término que generan varios puntos de elección (por ejemplo, con member, generará un punto por cada elemento de la lista). El segundo parámetro es un término que tiene que ser verdadero para todos los puntos de elección.

\begin{lstlisting}
?- List = [1,3,5,7], forall(member(X, List), 1 is X mod 2).
List = [1, 3, 5, 7].

?- List = [1,3,5,7], forall(member(X, List), 0 is X mod 2).
false.
\end{lstlisting}

El término findall nos permite encontrar todos los posibles resultados de un término. Para ello, primero indicaremos la variable que vamos a tratar de obtener todos sus valores, después el término en sí que genera soluciones diferentes para las variables y por último la lista donde se van a almacenar las soluciones.

\begin{lstlisting}
?- findall(Name, human(Name), Names).
Names = [socrates, kant].
\end{lstlisting}

Por último, maplist nos permite realizar un map sobre una lista. En primera posición marcamos el término que se va a aplicar para cada elemento, con N parámetros. A continuación N listas correspondientes a los N parámetros. Pueden ser tanto de entrada como de salida.

\begin{lstlisting}
?- maplist(write, [1,2,3]).
123
true.

?- maplist(sum, [[1,2,3],[4,5,6]], OutList).
OutList = [6, 15].
\end{lstlisting}

Otros metapredicados de interés pueden ser include, findnsols, bagof o setof.

\subsection{RDF}

Por último, vamos a comentar las facilidades que tiene Prolog para trabajar con RDF.

El término esencial es rdf, que representa el estado de las tripletas. rdf\_assert y rdf\_retractall permiten añadir y eliminar tripletas. También admite un sistema de prefijos similares a otros formatos de RDF. También existen términos para trabajar con RDF Schema como rdfs\_class\_property (para comprobar, generar las propiedades asociadas a una clase) o rdfs\_individual\_of (para comprobar si un recurso es individuo de una clase).

Además, la librería de RDF soporta diferentes tipos de literales mediante la sintaxis ^^. Estos tipos suelen estar definidos bajo la ontología de XML Schema, que ya hemos mencionado antes.

\begin{adjustbox}{width=\textwidth,keepaspectratio}
\begin{lstlisting}
Welcome to SWI-Prolog (threaded, 64 bits, version 8.1.29)
SWI-Prolog comes with ABSOLUTELY NO WARRANTY. This is free software.
Please run ?- license. for legal details.

For online help and background, visit https://www.swi-prolog.org
For built-in help, use ?- help(Topic). or ?- apropos(Word).

?- use_module(library(semweb/rdf11)).
true.

?- rdf_register_prefix(foaf, 'http://xmlns.com/foaf/0.1/').
true.

?- rdf_assert('http://example.com/Cervantes', foaf:name, "Miguel de Cervantes Saavedra").
true.

?- rdf(S,P,O).
S = 'http://example.com/Cervantes',
P = 'http://xmlns.com/foaf/0.1/name',
O = "Miguel de Cervantes Saavedra"^^'http://www.w3.org/2001/XMLSchema#string'.

?- rdf(S,P,O^^xsd:string).
S = 'http://example.com/Cervantes',
P = 'http://xmlns.com/foaf/0.1/name',
O = "Miguel de Cervantes Saavedra".

?- 
\end{lstlisting}
\end{adjustbox}


\chapter{Estado del Arte}

Antes de adentrarnos en los detalles de Lyncex conviene recapitular ideas similares ya existentes así como herramientas que nos puedan ayudar. 

\section{Frameworks web}

\subsection{Django}
Django es uno de los frameworks web más populares dentro del mundo Python y es una referencia dentro del mundo de los frameworks web por su potencia y claridad de código.\cite{django}
Fuertemente inspirado por \textbf{Ruby on Rails}, adopta de él su filosofía DRY (Don't Repeat Yourself) y su \textit{convention over configuration}.
El framework trabaja con bases de datos relacionales principalmente, pero es una buena base para diseñar la semántica del servidor web.

Django sigue una arquitctura MTV, es decir, Model-Template-View. Es bastante similar al popular MVC, aunque según sus creadores, el controlador es el propio framework.
En Django el modelo se define como una clase de Python, que gracias al ORM, tiene persistencia en la base de datos. La parte de las plantillas se programa con un lenguaje similar a HTML, pero con capacidad de mostrar variables
y de cierta lógica (condicional, bucle) con metaetiquetas. La parte de vista es código Python puro. Existe un fichero especial llamado urls.py que contiene un listado que relaciona las URLs con las vistas.

Las vistas pueden acceder a los parámetros GET o POST directamente y operar sobre ellos así como a la sesión del usuario que visita la página.
Las vistas en Django pueden protegerse mediante un sistema de autenticación sencillo pero seguro, que permite bloquear fácilmente o redirigir si el usuario no ha iniciado sesión o si no tiene los permisos suficientes.
Estas características de Django tienen el denominador común de que son implementadas usando middleware, es decir, componentes del framework que se introducen entre la petición bruta original
y la vista.

Otra característica interesante de Django apoyada en el middleware es la gestión de formularios. Los formularios se definen en Django como una clase más, parecida a un modelo de base de datos (de hecho, se puede hacer que sea la misma clase).
Con esta clase Django puede generar parte de la plantilla automáticamente. Posteriormente, en el método POST, el middleware puede limpiar y validar los campos, de modo que a la vista llegan ya los datos limpios.

Por último, existe un componente llamado \textbf{Django REST Framework}, que añade funcionalidad para diseñar una API REST con los mismos patrones y facilidades que el desarrollo web HTML.

\section{Bases de datos}
\subsection{Apache CouchDB}
Apache CouchDB es una base de datos NoSQL de tipo documental, almacenados estos en formato JSON.\cite{couchdb}
Su peculiaridad respecto a otras bases de datos similares como MongoDB viene por la forma de acceder a los datos.
Aunque en las últimas versiones CouchDB también dispone de un lenguaje de consulta, la forma original de acceder a la información era programando lo que se denominan vistas, en JavaScript.
Estas vistas generan un endpoint HTTP, que al ser llamado ejecutan el código JavaScript, en dos pasos. Primero un paso de map, para seleccionar los datos y realizar alguna transformación individual.
Posteriormente un paso de reduce permite realizar agregaciones. El código JavaScript que se ejecuta se almacena en la base de datos junto al resto de documentos.

Para interactuar con la base de datos se dispone de una API HTTP, tanto para crear como para borrar y o modificar los documentos.
Además dispone de un sistema de replicación entre nodos eventualmente consistente y relativamente simple, de forma que hay otros proyectos que implementan el mismo protocolo (PouchDB para navegadores web por ejemplo).

\section{Triplestores}

En esta sección analizaremos las bases de dato del tipo Triplestore. Estas bases de datos son muy similares, por no decir conceptualmente iguales, a las bases de datos de grafos, sin embargo, solo vamos a analizar triplestores que soporten explícitamente el modelo de RDF.

\subsection{TerminusDB}
TerminusDB es una base de datos programada en Prolog/Rust que comparte junto a CouchDB la característica de ser de tipo NoSQL.\cite{terminusdb}
Sin embargo, TerminusDB almacena tripletas RDF.
TerminusDB dispone de un esquema basado en OWL, muy potente y extendido por ellos mismos, que permite modelar ontologías muy completas, con relaciones avanzadas y restricciones potentes. Además incluye unas vistas basada en el concepto de documento, que generan formularios para el administrador y una API que permite interactuar en bloque con ciertas tripletas, tal y como si fuese un documento de CouchDB.

Las consultas se realizan mediante un lenguaje llamado WOQL, basado en JSON-LD. Además existen librerías para JavaScript y Python.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{terminusdb.png}
    \caption{Pantalla de administración de TerminusDB}
    \label{fig:terminusdb}
\end{figure}

Un ejemplo de WOQL.js y de WOQL en JSON-LD que muestra todas las tripletas cuyo sujeto es de tipo scm:Book.

\begin{lstlisting}[language=Java]

// WOQL.js
WOQL.and(
    WOQL.triple("v:Subject","type","scm:Book"),
    WOQL.opt().triple("v:Subject","v:Property","v:Value")
)

// WOQL
{
  "and": [
    {
      "triple": [
        "v:Subject",
        "rdf:type",
        "scm:Book"
      ]
    },
    {
      "opt": [
        {
          "triple": [
            "v:Subject",
            "v:Property",
            "v:Value"
          ]
        }
      ]
    }
  ]
}
\end{lstlisting}

\subsection{Apache Jena}
Apache Jena es una base de datos de tripletas RDF, una de las más maduras si nos atenemos a su longevidad (Jena 1.0 es del año 2000).\cite{jena}
Se trata de un proyecto con diversos componentes. Por un lado una API en Java para tratar con RDF, un motor de alto rendimiento SPARQL (ARQ) con soporte al estándar SPARQL Update, un almacenamiento persistente llamado TDB y un servidor que combina todo a través de una API, Fuseki. Además dispone de una API para tratar con ontologías OWL y una API de inferencias para usar diferentes razonadores RDF Schema y OWL. Las búsquedas sobre texto pueden realizarse usando por debajo el motor Apache Lucene, usado en otras bases de datos de búsqueda como Elasticsearch.


\subsection{Virtuoso y Marklogic}
OpenLink Virtuoso y MarkLogic son bases de datos multimodelo, entre ellos soportan el almacenamiento de tripletas.\cite{virtuoso}\cite{marklogic}.
Virtuoso tiene una versión opensource usada detrás de grandes silos de conocimiento como DBPedia o Wikidata. Ambas soportan queries vía SPARQL.

\subsection{ClioPatria}
ClioPatria es una aplicación que combina las librerías de RDF y HTTP de SWI-Prolog para ofrecer una base de datos semántica completa.\cite{cliopatria}
Soporta queries en SPARQL, SeRQL y en código Prolog. Al estar basada la aplicación en componentes independientes, podemos usarlos como base de nuestra aplicación.

\section{Análisis}

Esta tabla representa como cumplen las historias de usuario cada uno de los programas analizados.


\begin{table}[ht]
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{llllllll}
                  & \textbf{CouchDB} & \textbf{TerminusDB} & \textbf{Django} & \textbf{Jena} & \textbf{Virtuoso} & \textbf{ClioPatria} & \textbf{Lyncex} \\
    \textbf{Almacén tripletas}  & No               & Sí                  & Sí (1)          & Sí            & Sí            & Sí                  & Sí              \\
    \textbf{Ontología disponible}  & No               & No                  & No              & No            & No                & No                  & Sí              \\
    \textbf{Webs estáticas}  & Sí               & No                  & Sí              & No            & No                & No                  & Sí              \\
    \textbf{Webs plantillas}  & Sí               & No                  & Sí              & No            & No                & No                  & Sí              \\
    \textbf{Webs CRUD}  & Sí               & No                  & Sí              & No            & No                & No                  & Sí              \\
    \textbf{Autenticación} & Sí               & No                  & Sí              & No            & No                & No                  & No              \\
    \textbf{APIs} & Sí               & No                  & Sí (2)          & No            & No                & No                  & No              \\
    \textbf{SPARQL} & No               & No                  & No              & Sí            & Sí                & Sí                  & No             
    \end{tabular}
    }
    \caption{Comparativa de características entre diferente software}
    \label{table:comparativa}
\end{table}

Notas:
\begin{enumerate}
    \item Aunque Django cuenta con un ORM muy potente para trabajar sobre el modelo relacional, nada impide usar otros modelos de almacenamiento.
    \item A través de Django REST Framework, un componente independiente pero altamente acoplado
\end{enumerate}

Podemos concluir que no existe ningún software que cumpla el objetivo que nos hemos marcado al inicio del proyecto. Sin embargo, existe software que implementa en cierta medida muchas características que serían deseables en este sistema.

\chapter{Planificación}

En este capítulo hablaremos sobre la metodología Scrum, primero de forma genérica y luego adaptada a nuestro proyecto en particular. Además, veremos el plan de riesgos.
\section{Scrum}

\subsection{Introducción}
Scrum es una metodología ágil creada por Ken Schwaber y Jeff Sutherland a principios de los años 90.\cite{scrum}

Scrum fue diseñado para que equipos pequeños pudiesen desarrollar productos complejos de forma efectiva, adaptándose a los cambios de requisitos del producto.
El pilar fundamental de Scrum es el empirismo. Esta metodología sostiene que la única forma de obtener información correcta y precisa es mediante la experiencia, sin embargo, muchas metodologías de gestión de proyectos apenas tienen en cuenta la retroalimentación constante que genera el trabajo que se realiza semana a semana. Los pilares sobre los que se fundamenta Scrum se pueden resumir en: transparencia, inspección y adaptación.

\subsection{Actores}
En Scrum se definen varios actores dentro del proyecto: Product Owner, Scrum Master y equipo de desarrollo. Todos ellos forman parte del equipo Scrum, el cual es independiente y flexible en su forma de trabajar.

El \textbf{Product Owner} es el encargado de organizar el Product Backlog. Tiene que expresar claramente las tareas que hay que realizar, de forma transparente y asegurándose que el equipo de desarrollo entiende las tareas. Además debe priorizar las tareas de forma que maximice el valor del producto final y teniendo en cuenta el desempeño de los miembros del equipo de desarrollo. Define las condiciones para que una tarea pueda ser clasificada como completada.

El \textbf{equipo de desarrollo} es el equipo de profesionales que debe completar las tareas. Ante un Sprint backlog, deben organizarse entre sí para poder completar la mayoría de tareas allí descritas. Los equipos deben ser multidisciplinares, y aunque haya personas especializadas en algún punto, es el equipo como conjunto el que responde ante las tareas. El tamaño de los equipos ideal estaría entre 3 y 9 personas.

El \textbf{Scrum Master} es la persona encargada de promover el uso de Scrum y de aconsejar a los integrantes del equipo de formas de actuar válidas en Scrum.

\subsection{Eventos}

El evento principal en Scrum es el sprint. Los sprints son periodos de tiempo, de duración inferior a un mes (habitualmente dos semanas), donde se mejora el valor del producto de forma incremental. Los sprints comienzan justo cuando ha acabado el anterior. Durante el sprint no se debe modificar el objetivo del sprint y debe ser considerado como un proyecto independiente. Dentro de los sprints existen varios eventos.

El \textbf{sprint planning} es la reunión de todo el equipo, moderada por el Scrum Master que tiene lugar al inicio del sprint. En esta reunión se tienen que decidir de forma consensuada que tareas van a formar parte del Sprint Backlog (y serán realizadas en ese sprint). En primer lugar se debe tener claro el objetivo del sprint, las tareas seleccionadas del Product Backlog para formar parte del Sprint Backlog deben ser tareas que acerquen a todo el equipo a mejorar el objetivo. Además deben ser tareas realizables. Posteriormente, el equipo de desarrollo debe discuitir como va a realizar las tareas, pidiendo ayuda si hace falta al Product Owner para que especifique.

Cada día dentro del sprint el equipo de desarrollo tendrá un \textbf{daily scrum}, una reunión de 15 minutos donde decidirá que va a hacer cada miembro del equipo durante las siguientes 24 horas.

Al acabar el sprint, tiene lugar los \textbf{sprint review} y \textbf{sprint retrospective}. La primera es una reunión donde se valora el trabajo realizado, lo restante, se reevalúan prioridades y se comentan posibles dificultades que haya podido haber y en general, cualquier cosa que pueda servir para la mejora del producto.

La segunda reunión es mucho más introspectiva, y el objetivo es la mejora del equipo, como hacer que trabaje mejor, problemas interpersonales, etc

\subsection{Artefactos}

Durante un proyecto Scrum existen diversos artefactos. Siguiendo los valores fundacionales, estos deben ser transparentes, sujetos a la inspección y adaptables a las circunstancias.

El \textbf{Product Backlog} es una lista ordenada de tareas que se deben realizar en el proyecto. Es responsabilidad del Product Owner gestionar esta lista. La lista está ordenada según la prioridad de las tareas. La lista es dinámica y nunca está completa. Continuamente se van añadiendo tareas (funcionalidad nueva, errores, pruebas, ...) y es un reflejo de como el producto evoluciona. Es muy recomendable que las tareas, además de ser claras, incluyan una definición de "completo" para saber de forma objetiva cuando una tarea ha sido finalizada.

El \textbf{Sprint Backlog} es un conjunto de tareas seleccionadas del Product Backlog más tareas que surgen durante el propio sprint. Todas ellas deben ser mencionadas en los daily scrum. Solo el equipo de desarrollo puede modificar el sprint backlog una vez ha comenzado el sprint y siempre y cuando no modifique el objetivo del sprint. Idealmente, todas las tareas del sprint backlog deberían estar completas al acabar el sprint.

El \textbf{incremento} son las tareas finalizadas al acabar el sprint y que incrementan el valor del producto. El incremento debe ser algo inspeccionable y un paso adelante hacia el objetivo. El incremento debe estar en condiciones de uso aunque el Product Owner puede decidir no publicar una nueva versión.

\section{Metodología de trabajo}
Para el proyecto se ha tratado de seguir una metodología similar a Scrum, obviando ciertas diferencias inherentes al trabajo invididual.

En primer lugar se fija el tiempo de los sprints a 3 semanas. Tiempo suficiente para avanzar de forma significativa y poder acabar bastantes tareas, sin con ello perder el foco para poder iterar en caso de que haya que hacer cambios o cierta parte se quede atascada. No habrá dailies, aunque existirá una comunicación constante mediante la herramienta Slack con los tutores por si hubiese dudas puntuales.

El product backlog se define en un tablero de Trello, donde se anotan todas las tareas pendientes, más grandes o más pequeñas. Estas tareas son definidas por mí mismo, haciendo el rol de Product Owner.

Al principio del sprint tiene lugar una reunión que combina el sprint review y el sprint planning. Esta reunión se realizaba principalmente usando la herramienta Google Hangouts. Primero se repasaba el sprint backlog del sprint anterior, para ver qué se movido al incremento. Además, se mencionaban ciertos problemas que habían sido encontrados: problemas en la definición de completado, dificultades a la hora de modelar ciertos aspectos, bugs pertenecientes a librerías que usamos, etc.

Además se repasa la parte del sprint backlog que no ha sido completada. Esta se repasa para ver los motivos de que no hubiese podido ser complatada y si tiene sentido seguir con ella en el siguiente sprint o si es mejor eliminarla.

Una vez acabada esta parte, comienza la parte de sprint planning, donde se seleccionan las tareas del product backlog que pasan al sprint backlog para el sprint siguiente. Además se analizan por encima como se piensan implementar, aunque esto no es concluyente en ningún caso.

Durante la reunión estan presentes uno o dos tutores, desempeñando ellos un rol de Scrum Master (Miguel Ángel Martínez Prieto y María Aránzazu Simón Hurtado).

El resto del sprint, el equipo de desarrollo (yo mismo) va modificando las tareas sobre Trello según se cumplen las condiciones de completado o si se encuentra algún obstáculo.

\section{Plan de gestión de riesgos}

Para estar prevenidos antes posibles riesgos que pudieran darse en la ejecución de este proyecto, se planifica con antelación ciertos problemas que podrían surgir. Este plan se elabora en cuatro fases:
\begin{enumerate}
    \item Identificación de los riesgos
    \item Análisis y priorización de los riesgos
    \item Planificación sobre los riesgos
    \item Monitorización de los riesgos
\end{enumerate}

Para priorizar los riesgos tendremos en cuenta la siguiente expresión:
\begin{math}
RE = d*p
\end{math}
donde RE es el Risk Exposure, d es el daño potencial que puede causar y p es la probabilidad de que suceda. El daño puede medirse como una cuantía económica, o en nuestro caso, en una escala de 0 a 10. Las probabilidades usan la misma escala.

\begin{table}[]
\resizebox{\textwidth}{!}{\begin{tabular}{lllll}
\textbf{ID} & \textbf{Descripción}                             & \textbf{Daño} & \textbf{Probabilidad} & \textbf{RE} \\
R1          & Pandemia global                                  & 5             & 0.5                   & 2.5         \\
R2          & Enfermedad desarrollador                         & 7             & 2                     & 14          \\
R3          & Bug inesperado en las librerías                  & 4             & 4                     & 16          \\
R4          & Equipos se estropean                             & 9             & 1                     & 9           \\
R5          & Se encuentra fallo grave de diseño tarde         & 6             & 3                     & 18          \\
R6          & La implementación tarda más de lo previsto       & 6             & 5                     & 30          \\
R7          & Cambian los requisitos durante la implementación & 7             & 5                     & 35          \\
R8          & Historias de usuario poco definidas              & 3             & 5                     & 15          \\
R9          & Tests poco realistas                             & 4             & 4                     & 16          \\
R10         & Conocimiento de tecnologías insuficiente         & 2             & 6                     & 12         
\end{tabular}}
\caption{Análisis de riesgos}
\label{table:risks-analysis}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{risk-matrix.png}
    \caption{Matriz de riesgos}
    \label{fig:risk-matrix}
\end{figure}

Una vez realizado este análisis, planificamos qué hacer con cada uno de ellos. Las opciones disponibles son:

\begin{itemize}
    \item Aceptar que los riesgos puedan ocurrir
    \item Evitar el riesgo
    \item Reducir el riesgo
    \item Transferir el riesgo
    \item Mitigar el riesgo
\end{itemize}

La diferencia principal entre evitar/reducir y mitigar un riesgo es que en el primer caso, hemos de realizar una inversión aunque el riesgo finalmente no se materialice, mientras que en el segundo caso, solo deberíamos dedicarle atención una vez sucede.

\begin{table}[]
\resizebox{\textwidth}{!}{\begin{tabular}{lll}
\textbf{ID} &
    \textbf{Descripción} &
    \textbf{Plan} \\
R1 &
    Pandemia global &
    Aceptar el riesgo \\
R2 &
    Enfermedad desarrollador &
    Aceptar el riesgo \\
R3 &
    Bug inesperado en las librerías &
    \begin{tabular}[c]{@{}l@{}}Reducir el riesgo tratando de usar \\ la menor cantidad de librerías posibles\end{tabular} \\
R4 &
    Equipos se estropean &
    \begin{tabular}[c]{@{}l@{}}Mitigar el riesgo usando otros PCs de casa y\\ reducir el riesgo manteniendo copias de seguridad\end{tabular} \\
R5 &
    Se encuentra fallo grave de diseño tarde &
    \begin{tabular}[c]{@{}l@{}}Reducir y transferir el riesgo haciendo sprints\\ reviews y plannings junto al tutor\end{tabular} \\
R6 &
    La implementación tarda más de lo previsto &
    \begin{tabular}[c]{@{}l@{}}Mitigar el riesgo teniendo siempre entregables\\ listos para presentar al final de cada sprint\end{tabular} \\
R7 &
    Cambian los requisitos durante la implementación &
    \begin{tabular}[c]{@{}l@{}}Reducir el riesgo introduciendo un sistema de\\ testing flexible y polivalente\end{tabular} \\
R8 &
    Historias de usuario poco definidas &
    \begin{tabular}[c]{@{}l@{}}Reducir y transferir el riesgo haciendo sprints\\ reviews y plannings junto al tutor\end{tabular} \\
R9 &
    Tests poco realistas &
    \begin{tabular}[c]{@{}l@{}}Reducir el riesgo introduciendo un sistema de\\ testing flexible y polivalente\end{tabular} \\
R10 &
    Conocimiento de tecnologías insuficiente &
    \begin{tabular}[c]{@{}l@{}}Evitar el riesgo estudiando más sobre Prolog \\ \\ y RDF.\end{tabular}
\end{tabular}}
\caption{Plan de riesgos}
\label{table:risks-plan}
\end{table}

% Merece especial mención la historia de usuario de desarrollo de ontología. Si bien podría ser un requisito previo de cara a otras historias de usuario, en la práctica
% se han desarrollado de forma paralela, añadiendo lo necesario a la ontología en cada historia de usuario nueva.

% Cada una de estas historias de usuario tenía asociadas diferentes condiciones de aceptación. Se trata de frases que siguen el estilo: Dado/Cuando/Entonces. 
% Una implementación completa de la historia de usuario hará que todas las frases sean ciertas. Gracias a Behave, estas condiciones de aceptación no solo existen en el papel, sino en el código también que comprueba de forma activa además que no haya regresiones en etapas posteriores.

\chapter{Análisis y diseño}

En este capítulo realizaremos el análisis del proyecto a realizar. Primero explicaremos en detalle las historias de usuario ya mencionadas en el primer capítulo. Después analizaremos los requisitos que surgen de esto, tanto funcionales como no funcionales.

\section{Historias de usuario}

Las historias de usuario que se han seleccionado para este proyecto son las siguientes:
\begin{enumerate}
    \item Almacenar datos junto a sus ontologías en el mismo espacio, validándose. El objetivo es tener un sistema de almacenamiento de tripletas, que sirva de base para el resto de historias. Este almacenamiento deberá ser compatible con tripletas, preferiblemente en formato Turtle y deberá ser manipulable desde el exterior también con operaciones de búsqueda, inserción y eliminación de datos.
    \item Desarrollar una ontología que permita definir aplicaciones web sobre datos almacenados en el propio almacenamiento. Esta ontología deberá aprovechar otras ontologías existentes, en la medida de la posible. La ontología deberá definirse usando tecnologías interoperables. Se deja la puerta abierta a que se defina una ontología más amplia de la que pueda manejar Lyncex al finalizar el proyecto.
    \item Servir páginas web estáticas definidas mediante esa ontología. Se entiende por páginas estáticas aquellas cuyo contenido no depende más que de si mismo.
    \item Servir páginas web que necesiten leer datos del almacenamiento (plantillas). Estas páginas son aquellas que implementan un procesamiento extra (plantillas) y además pueden leer y escribir datos en en el almacenamiento. Además pueden acceder a valores de parámetros GET y POST y pueden realizar validaciones.
    \item Servir páginas web CRUD con formularios. Los formularios son una abstracción para la manipulación de datos que facilita el trabajo en datos agrupados sobre un sujeto. Esta abstracción deberá permitir realizar formularios automáticamente en HTML, validarlos. Editar y borrar su contenido. Además habrá páginas públicas y páginas privadas protegidas por un usuario/contraseña.
\end{enumerate}

\section{Requisitos}

A continuación analizamos los requisitos que surgen de estas historias de usuario.

\subsection{Requisitos funcionales}
\begin{enumerate}
    \item El sistema deberá ser capaz de almacenar tripletas RDF.
    \item El sistema deberá permitir consultar todas las tripletas almacenadas de forma externa mediante una API.
    \item El sistema deberá permitir consultar las tripletas almacenadas aplicando filtros de forma externa mediante una API.
    \item El sistema deberá ser capaz de admitir tripletas nuevas de forma externa mediante una API.
    \item El sistema deberá ser capaz de eliminar tripletas de forma externa mediante una API.
    \item El sistema deberá ser capaz de eliminar tripletas aplicando filtros de forma externa mediante una API.
    \item El sistema deberá validar que la propiedad rdfs:range definida en las ontologías residentes en el almacenamiento se cumple.
    \item El sistema deberá mostrar un error cuando la ruta especificada no concuerde con ninguna página.
    \item El sistema deberá mostrar contenido estático de tipo textual.
    \item El sistema deberá mostrar contenido estático de tipo binario codificados en Base64.
    \item El sistema deberá diferenciar entre peticiones GET y POST.
    \item El sistema deberá disponer de un motor de plantillas con condicionales.
    \item El sistema deberá disponer de un motor de plantillas con bucles.
    \item El sistema deberá disponer de un motor de plantillas que permita imprimir valores del almacenamiento.
    \item El sistema deberá poder leer parámetros GET y exponerlos en las plantillas.
    \item El sistema deberá poder leer parámetros POST y exponerlos en las plantillas.
    \item El sistema deberá poder ejecutar validaciones de expresiones regulares sobre el contenido de los parámetros.
    \item El sistema deberá poder ejecutar validaciones Prolog sobre el contenido de los parámetros. 
    \item El sistema deberá poder ejecutar código Prolog para resolver las variables de salida de la plantilla
    \item El sistema deberá generar un formulario HTML automáticamente basado en una ontología
    \item El sistema deberá poder validar los formularios con expresiones regulares
    \item El sistema deberá poder validar los formularios con código Prolog
    \item El sistema deberá poder guardar directamente los formularios
    \item El sistema deberá permitir visualizar el contenido de los formularios ya guardados
    \item El sistema deberá poder editar formularios ya almacenados
    \item El sistema deberá poder borrar el contenido de formularios ya almacenados
    \item El sistema deberá poder restringir el acceso a una página mediante usuario y controseña
\end{enumerate}

\subsection{Requisitos no funcionales}
\begin{enumerate}
    \item El sistema deberá estar programado en Prolog
    \item El sistema deberá funcionar sobre Linux
    \item El sistema deberá soportar la codificación UTF-8 en todas partes
    \item La API de comunicación externa será con verbos HTTP
    \item La API de comunicación externa usará el formato Turtle
    \item El sistema deberá poder ser instalable por el usuario en menos de 30 minutos
    \item El sistema debe ser fiable en todo momento
\end{enumerate}

\section{Diseño de Lyncex}

\subsection{Arquitectura}
En primer lugar debemos tener en cuenta que Lyncex es un sistema preparada para trabajar en un modelo cliente-servidor, donde el cliente es un navegador web. Esto nos permite partir en varias capas. Sin embargo, la capa de presentación es muy ligera y depende mucho del templatizado que realice el usuario. La capa de negocio es donde se implementa prácticamente toda la aplicación y la capa de datos es donde tiene lugar la persistencia.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{plantuml/front-controller.png}
    \caption{Arquitectura de Lyncex}
    \label{fig:arquitectura}
\end{figure}

Entrando dentro de la capa de negocio tenemos la aplicación del patrón Front Controller.
Este patrón, diseñado para aplicaciones web donde existe un único punto de entrada. Este realiza tareas comunes y finalmente delega la acción concreta a un controlador. Alrededor de los controladores existen helpers que implementan funcionalidad común.

\subsection{API}

La API de Lyncex se encarga de que el sistema interactúe con el exterior.
Un programador que desee crear una aplicación de Lyncex es lo único que debería tocar.
Externamente se trata de una API HTTP, que define varias rutas bajo la ruta \_api. La barra baja se ha elegido para no interferir con un posible sistema de APIs de la aplicación que use Lyncex.
Las operaciones que soporta la API son:
\begin{itemize}
    \item Creación de tripletas en la base de datos (POST, \_api)
    \item Borrado de tripletas de la base de datos (DELETE, \_api/delete) con posibilidad de aplicar filtros
    \item Lectura de tripletas de la base de datos (GET, \_api/query) con posibilidad de aplicar filtros
\end{itemize}

La creación de tripletas toma como entrada un fichero de tipo Turtle, mismo formato que se encuentra a la salida de la lectura de tripletas.
De esta forma, se pueden realizar backups rápidos de la aplicación.

Los filtros se implementan mediante parámetros GET.

El componente API además dispone de una validación elemental de RDF Schema.
Esto se consigue si tanto los datos como las ontologías que los definen coexisten en la aplicación.
Se verifica básicamente el uso correcto de instancias y propiedades mediante rdfs:range.
Técnicamente, RDF Schema no es un lenguaje de validación al uso, sino más bien de descripción de datos, pero demasiado abierto como para hacer comprobaciones estrictas.
Es por ello que solo se valida este único comportamiento.

\section{Diseño de la ontología}

\begin{sidewaysfigure}[h]
    \centering
    \includegraphics[width=\textwidth]{lyncex.png}
    \caption{Ontología de Lyncex}
    \label{fig:ontologia}
\end{sidewaysfigure}

\subsection{Controladores}
Los controladores son los componentes que implementan la funcionalidad principal definida por la ontología.
Al llegar una petición, se van probando los diferentes tipos de controladores hasta encontrar uno que sea del tipo al definido en la tripleta: (Controlador, rdf:type, lyncex:TipoControlador).
Inicialmente se han diseñado tres controladores.

\subsubsection{ContentController}
El más básico de todos, simplemente devuelve lo que tenga definido en la tripleta (Controlador, lyncex:content, Content) con el tipo MIME 
de la tripleta (Controlador, lyncex:mime, MimeType). El nodo Content es del tipo ContentAsText o ContentAsBase64, permitiendo ambos modos de representación del contenido.
Ambos tipos forman parte de la ontología de W3C llamada \textit{Representing Content in RDF 1.0}\cite{cnt}.
El tipo ContentAsText está pensado para formatos de tipo texto (HTML, CSS, JavaScript) mientras que ContentAsBase64 está pensado para formatos binarios (imágenes, sonidos, etc).
Hay que mencionar, que en ningún caso almacenar ficheros binarios codificados en base 64 es una opción óptima y esta opción se ofrece más como una conveniencia.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{plantuml/contentcontroller.png}
    \caption{Diagrama de secuencia del ContentController}
    \label{fig:contentcontroller}
\end{figure}

\subsubsection{TemplateController}
El TemplateController se encarga de gestionar las plantillas. Estas plantillas se definen siguiendo la sintaxis Semblance de la librería simple-template.
El TemplateController realiza las siguientes operaciones en orden:
\begin{enumerate}
    \item Procesado de parámetros GET y POST definidos previamente por instancias lyncex:Parameter
    \item Templatizado de queries (si lo hubiera) especificadas (instancias de lyncex:Query)
    \item Resolución de las queries (si hubiera)
    \item Ejecución de los handlers (si hubiera) (instancias de lyncex:Handler)
    \item Templatizado final
\end{enumerate}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{plantuml/templatecontroller.png}
    \caption{Diagrama de secuencia del TemplateController}
    \label{fig:templatecontroller}
\end{figure}

\subsubsection{FormController}
El FormController es una abstracción por encima del TemplateController a nivel interno, pero de cara a la ontología no es una subclase.
Su funcionamiento depende de si se realiza una petición POST o GET. Ante una petición POST, el controlador procesará los parámetros.
Buscará la existencia de un parámetro llamado \_id en primer lugar, ya que definirá el sujeto sobre el que se van a almacenar tripletas.
A continuación recorrerá el resto de parámetros. El nombre de cada parámetro es la URL de la propiedad y el valor será el valor de la propiedad.
Actualmente no se soporta crear dos tripletas de la misma propiedad en el mismo procesado.

Si tenemos una operación GET, el controlador leerá la clase base y todas las propiedades que pueda extraer de la clase. Para ello es importante que la ontología de los datos esté cargada, si no, no será capaz de adivinar qué propiedades admite la clase.
Una vez tenga el listado de propiedades, generará un formulario HTML ajustando todos los valores de forma adecuada al formato de aceptación del POST.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{plantuml/formcontroller-get.png}
    \caption{Diagrama de secuencia del FormController ante un GET}
    \label{fig:formcontrollerget}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{plantuml/formcontroller-post.png}
    \caption{Diagrama de secuencia del FormController ante un POST}
    \label{fig:formcontrollerpost}
\end{figure}

\subsubsection{LoginController}

\subsection{Procesado de parámetros}
El procesado de parámetros es llamado por diferentes controladores para adaptar el formato de entrada HTTP GET y POST a un formato común (un diccionario Prolog).
Solo saldrán del procesado aquellos parámetros indicados de forma explícita, ignorando aquellos que no lo estén.
Durante el procesado se ejecutan las validaciones si las hubiera. Existen dos tipos de validaciones: Regex y Prolog.
Cualquier parámetro puede tener cero, una o ambas validaciones.

La validación regex, simplemente comprueba que el valor del parámetro cumpla con el patrón de una expresión regular estándar. Por debajo se implementa mediante la librería PCRE.

La validación Prolog es código Prolog que define un término validation(X). Dentro de este código se puede llamar a cualquier código Prolog.
La validación será exitosa si el término se puede evaluar a true, en caso contrario, se considerará que el parámetro está mal y fallará.

En el caso del FormController, las validaciones tienen que ubicarse en otro lugar. Ya que el FormController importa implícitamente todos los parámetros de una ontología, no se puede adjutar una validación de la misma forma.
La solución si usamos un FormController es extender la propia ontología.
Así, en un sujeto de tipo rdf:Property, podemos añadir lyncex:validation y lyncex:code\_validation, que funcionan como validación regex y validación Prolog, respectivamente.

\chapter{Implementación}

\section{Herramientas de desarrollo}
\subsection{SWI Prolog}
SWI Prolog\cite{prolog} es un entorno de programación Prolog, ampliamente usado gracias a su condición de software libre, su portabilidad y 
su estrategia de baterías incluidas, que hace que disponga de gran cantidad de librerías y módulos por defecto.
La comunidad de SWI Prolog es pequeña pero activa y a parte de la gran cantidad de librerías, existe un sistema de packs, que permiten instalar librerías de terceros.

Entre los módulos que tiene SWI Prolog y que es difícil encontrar en otras implementaciones, debemos destacar los módulos de web semántica y RDF y los de HTTP.
Sin estos módulos, la realización de Lyncex hubiese sido mucho más compleja y alargada en el tiempo.

\subsection{IDE}
Como IDE se ha utilizado Visual Studio Code\cite{vscode}, un editor propiedad de Microsoft, pero multiplataforma.
Es un IDE con el que ya había familaridad previa y no supone ningún esfuerzo usarlo para este proyecto.
Visual Studio Code dispone de un sistema de plugins, y entre otros, hay plugins de Prolog y de Turtle.
El último fue usado para obtener resaltado de sintaxis, mientras que el primero se evaluó y finalmente se deshechó.
El motivo es que el plugin ejecuta Prolog por debajo para detectar errores de sintaxis, pero al tratarse de una aplicación que es un servidor, al ejecutarse toma los puertos y no podemos usarlos
en pruebas lanzadas a través de la terminal.

\subsection{Sistema operativo}
La totalidad del proyecto se ha realizado sobre Debian, en su versión Sid. Se trata de un sistema operativo con kernel Linux, de amplio uso en servidores, y también usado, en menor medida, en portátiles y workstations.

\subsection{Docker}
Para que los entornos de prueba y de ejecución sean reproducibles, se ha optado por usar Docker y su utilidad, Docker Compose.
Docker es un sistema de contenedores para Linux que proporciona capacidades de aislamiento y reproducibilidad de entornos similares a las máquinas virtuales, sin el overhead que usar estas conlleva. Con Docker podemos generar versiones del software inmutables y ejecutables en cualquier kernel Linux sin necesidad de ninguna dependencia más.
Además mejora mucho la seguridad por defecto de los procesos que se ejecutan en su interior.
Docker además permite ejecutar contenedores en MacOS y Windows mediante una virtualización transparente de Linux.
Existen otros sistemas parecidos, como LXD, pero Docker tiene varias ventajas: poder describir entornos como código versionable (Dockerfile) y poder orquestar de forma sencilla entornos pequeños y medianos (docker-compose).

El fichero Dockerfile (y Dockerfile.test para los tests E2E) es una definición de construcción de imágenes. Parte de una imagen base (FROM) y ejecuta comandos para construir una imagen derivada de esta (RUN, COPY) y realiza configuraciones (WORKDIR, CMD, USER). Para ejecutar el programa debemos construir un contenedor que parta de esa imagen inmutable. El contenedor puede realizar cambios en su sistema de archivos aislado, pero salvo que lo configuremos de forma explícita, estos cambios se perderán al borrar el contenedor. Los contenedores también se pueden parar y reanudar. Estas imágenes generadas quedan almacenadas por defecto en el repositorio local, pero se pueden subir a cualquier repositorio de Docker. Los más populares son Docker Hub, Quay.io y Azure Container Registry.

El fichero docker-compose.yml es una definición de los contenedores que tienen que lanzarse, con sus puertos, volúmenes, etc. Podemos basarnos en imágenes locales y también podemos sobreescribir ciertas configuraciones (como el comando). Además se pueden definir dependencias entre contenedores a la hora de arrancar.

\subsection{Git y GitHub}
Para versionar el código fuente se ha usado el sistema Git, usando como almacenamiento GitHub. Principalmente se ha usado a través de la línea de comandos.
No se han aprovechado muchas de sus características de ramas y merges, ya que al ser un proyecto personal, solo se iba modificando una parte del programa a la vez.
Adicionalmente se ha hecho uso de la integración continua gratuita ofrecida por GitHub llamada GitHub Actions. De este modo se ha definido una acción que se ejecuta al recibir nuevo código en el repositorio.
Esta acción construía las imágenes de Docker necesarias y pasaba los tests de Behave y de PlUnit.

\subsection{Behave}
Para asegurarnos que las historias de usuario se implementaban correctamente se ha decidido usar Behave.
Se trata de un framework para definir tests en lenguaje Gherkin, un lenguaje muy similar al lenguaje natural. Estos tests se componen de pasos,
los cuáles son implementados en Python. Como las historias de usuario nos hablan de como debe reaccionar el sistema con el exterior, no hay problema en implementar los tests de este tipo en otro lenguaje.

Más detalles sobre Behave en el capítulo de Validación y pruebas.


\chapter{Validación y pruebas}

Para comprobar la validez de la implementación se realizan pruebas de diversos tipos.

\section{Pruebas manuales}

El primer tipo de pruebas que se ha realizado son aquellas con intervención manual, realizadas por el programador. En estas principalmente se evalúa que el concepto sigue funcionando. Estas pruebas tienen poco valor, ya que son difíciles de replicar en idénticas condiciones.

No obstante, hay una prueba manual completa en forma de la aplicación de ejemplo, que detallaremos más en profundidad en el capítulo correspondiente.

\section{Tests unitarios}

Entendemos por tests unitarios aquellos que prueban de forma aislada una parte del código. En el proyecto se han implementado tests unitarios usando el framework PlUnit. Los tests unitarios de PlUnit tienen la siguiente estructura:

\begin{lstlisting}[language=Prolog]

:- begin_tests(api).

test(bnode) :-
    X = rdf(node(1), lyncex:content, node(2)),
    X = rdf(SOrg, _, OOrg),
    map_bnode(SOrg, OOrg, S, O),
    \+ S = node(_),
    \+ O = node(_).

:- end_tests(api).

\end{lstlisting}

Se definen zonas de test delimitadas por begin_tests y end_tests. En su interior definimos reglas test, que deben ser cumplirse. En este ejemplo, queríamos probar la regla map_bnode y si todo va bien, todas los términos deben ser verdaderos. La ejecución de los tests se detalla en el manual de instalación.

Para ejecutar los tests unitarios, el método recomendado es usar Docker Compose y ejecutar:

\begin{verbatim}
docker-compose run test-unit
\end{verbatim}

\section{Test E2E}

Los tests E2E prueban la validez del sistema en su conjunto (end-to-end, punto a punto). Esto se consigue levantando un servidor de Lyncex e insertando datos de aplicaciones de prueba. Posteriormente, vamos probando que todas las características funcionan tal y como se especificó en los tests. Estos tests se encuentran definidos en la carpeta features.

Estos tests son muy importantes ya que durante el desarrollo se definía cada tarea nueva como un test E2E. Si el código pasa el test creado con anterioridad, significa que la tarea está completada y pasa a formar parte del incremento.

Los test E2E se implementan con Behave, que es una herramienta específica para realizar Behaviour Driven Development (BDD). La idea es definir, en lenguaje natural, las condiciones de aceptación de una característica. Para ello usa las palabras Given/When/Then (y los conectores And/But), más conocido como lenguaje Gherkin. Las principales ventajas de este modelo es que podemos trasladar todas las condiciones de aceptación a código y que los tests los pueden escribir personas con escaso conocimiento técnico. El corolario de la primera consecuencia es que siempre vamos a estar atentos ante regresiones en el código.

Las frases con Given son precondiciones. Acciones que se realizan para dejar el sistema en un estado listo para ejecutar la prueba. Las frases con When (idealmente solo una frase), representan el sujeto a prueba, el estímulo que tenemos que hacer para desencadenar aquello que queremos probar. Finalmente, con Then deberíamos comprobar que el test arroja los resultados esperados. Deben ser principalmente aserciones.

Behave permite implementar los pasos en Python, en un fichero paralelo (carpeta steps).

Veamos un ejemplo de un test E2E.

\begin{lstlisting}
Scenario: Parameters (GET, valid)
    Given I have an empty Lyncex instance
    And I do a POST request with 'features/test3.ttl' data
    When I visit '/person3?id=42'
    Then I get a 200 status code
    And I get a '<b>ID: </b>42' response
    And I get a 'text/html' response type
\end{lstlisting}

Los steps son parecidos a esto:
\begin{lstlisting}
from behave import step
import requests
import hashlib

@step("I visit '{url}'")
def step\_visit\_url(context, url):
    context.request = requests.get(f"http://lyncex:11011{url}")

@step("I get the photo '{photo}'")
def step\_get\_photo(context, photo):
    m = hashlib.sha1()
    m.update(context.request.content)
    n = hashlib.sha1()
    with open(photo, "rb") as f:
        n.update(f.read())
    assert m.digest() == n.digest()
\end{lstlisting}

los steps admiten un templatizado inverso (sirven como patrón para sacar datos) y además siempre comparten una variable llamada context, que podemos usar para conservar estado entre steps.

Para ejecutar los tests E2E, el método recomendado es usar Docker Compose y ejecutar:

\begin{verbatim}
docker-compose run test-e2e
\end{verbatim}

\section{GitHub Actions}

GitHub Actions es un servicio de integración continua/despliegue continuo (CI/CD por sus siglas). Se encuentra integrado dentro del servicio web GitHub y nos permite definir tareas que se ejecutan ante ciertas acciones de entrada. Uno de los usos más habituales es la de comprobar, cuando llegan commits nuevos al repositorio Git, que los tests siguen pasando.

El proyecto cuenta con un fichero que activa GitHub Actions. Este fichero simplemente ejecuta los dos tipos de tests automatizados y si alguno de ellos finaliza incorrectamente, marca el commit en rojo y envía un correo para avisar de que hay código que no pasa los tests en el repositorio.

\begin{lstlisting}
name: Test
on: [push]
jobs:
    test-e2e:
        runs-on: ubuntu-18.04
        steps:
        - name: Checkout
          uses: actions/checkout@v1
        - name: Build Docker images
          run: docker-compose build
        - name: Execute E2E test
          run: docker-compose run test-e2e
    test-unit:
        runs-on: ubuntu-18.04
        steps:
        - name: Checkout
          uses: actions/checkout@v1
        - name: Build Docker images
          run: docker-compose build
        - name: Execute unit tests
          run: docker-compose run test-unit
\end{lstlisting}

\chapter{Aplicación de ejemplo}

Con el fin de demostrar que Lyncex es un framework útil, se ha decidido escribir una aplicación de ejemplo. Esta aplicación será una prueba manual de que el sistema funciona y servirá para validar conceptos.

\subsection{Objetivo y alcance}

La aplicación tendrá como objetivo poder editar el dataset de bibliotecas de Castilla y León. Este dataset, ya está expresado en formato RDF en el portal Datos Abiertos de la Junta de Castilla y León, usando dos ontologías comunes: vCard y FOAF.

La duración del proyecto no será mayor de una semana. No se necesita una interfaz de usuario refinada. El proyecto debe tratar de usar, de forma idiomática, el framework Lyncex.

Definimos las siguientes historias de usuario:
\begin{enumerate}
    \item Visualizar datos de las bibliotecas. Mostrar tanto en listado como en individual los datos de las bibliotecas.
    \item Editar datos de las bibliotecas. Crear, eliminar y editar datos de las bibliotecas. Restringir el acceso de edición a la cuenta del administrador.
    \item Mostrar bibliotecas en un mapa. Poner cada biblioteca en un mapa interactivo.
\end{enumerate}

\subsection{Análisis y Diseño}

Los requisitos que podemos sacar de las historias de usuario son los siguientes:

\subsubsection{Requisitos funcionales}
\begin{enumerate}
    \item El sistema deberá permitir visualizar los datos de una biblioteca individualmente
    \item El sistema deberá permitir visualizar un listado de las bibliotecas
    \item El sistema deberá permitir crear nuevas bibliotecas 
    \item El sistema deberá permitir eliminar bibliotecas
    \item El sistema deberá permitir editar datos de las bibliotecas
    \item El sistema deberá bloquear el acceso a las secciones de edición, dejando acceder al administrador
\end{enumerate}
\subsubsection{Requisitos no funcionales}
\begin{enumerate}
    \item El sistema deberá ejecutarse como una aplicación dentro de Lyncex
\end{enumerate}

\subsection{Arquitectura}

La arquitectura es básicamente un recubrimiento por encima de la de Lyncex. Se trata de una aplicación web con tres capas: presentación, negocio y datos. En las tres capas se delega parte del trabajo a Lyncex.

Los datos ya tienen definida dos ontologías que especifican como son los datos. En este caso son FOAF y vCard. La primera sirve para modelar interacciones sociales (personas y agentes) y la segunda modela una agenda de contactos. 

\begin{sidewaysfigure}
    \centering
    \includegraphics[width=\textwidth]{foaf.png}
    \caption{Diagrama de la ontología FOAF \\ \url{http://owlgred.lumii.lv/online_visualization/uo3g}}
    \label{fig:foaf}
\end{sidewaysfigure}

\begin{sidewaysfigure}
    \centering
    \includegraphics[width=\textwidth]{vcard.png}
    \caption{Diagrama de la ontología vCard \\ \url{http://owlgred.lumii.lv/online_visualization/2pt2}}
    \label{fig:vcard}
\end{sidewaysfigure}

\subsection{Componentes}

La aplicación se compone de dos componentes: un script de transformación de la entrada y por otro lado, la propia aplicación, que consideramos suficientemente pequeña como para ser un componente por sí mismo.

Aun así, podemos dividir la aplicación en diferentes páginas:
\begin{itemize}
    \item HomePage. La página de inicio
    \item BookPage. Vista individual de una biblioteca.
    \item BookListPage. Listado de todas las bibliotecas.
    \item LoginPage. Permite acceder al sistema como usuario administrador.
    \item BookForm. Formulario protegido para añadir/editar/borrar bibliotecas.
    \item GeoForm. Formulario protegido para añadir/editar/borrar puntos geográficos.
\end{itemize}

El script surge como consecuencia del primer requisito no funcional. Es necesario realizar una transformación de los datos a Turtle si no lo estuviesen. Los datos en la paǵina oficial de datos abiertos se encuentran en RDF, pero en su sintaxis XML. Es por ello necesario realizar un pequeño script que realice la transformación inicial.


\subsection{Implementación}

Para la implementación se han usado las mismas herramientas que para el desarrollo de Lyncex, es decir, Visual Studio Code, Git, y, evidentemente, RDF. El script se realizó con Python usando la librería rdflib.

La aplicación intenta usar todas las características presentes en Lyncex. 

Inicialmente define un prefijo para vCard. Posteriormente va definiendo las páginas como controladores de Lyncex. HomePage es un ContentController. BookPage y BookListPage son TemplateControllers, ambos usando handler, queries y parámetros. LoginPage es un LoginController y BookForm y GeoForm son FormController con control de acceso.

\subsection{Validación y pruebas} 

La aplicación de ejemplo no dispone de pruebas automatizadas, pero sí se han ejecutado pruebas manuales. Cabe destacar que la implementación de esta aplicación sacó a la luz bugs originales de Lyncex. Gracias a ello se implementó la función exists dentro del sistema de plantillas.

\subsection{Tour}

Para comprobar el funcionamiento de esta aplicación de ejemplo, vamos a hacer un tour de alto nivel. Suponemos que tenemos una instancia de Lyncex arrancada en localhost, en el puerto 11011 y está vacío el triplestore.

Lo primero será añadir las tripletas necesarias. Hay tres archivos:
\begin{itemize}
    \item vcard.ttl - Una miniontología RDF Schema que simula a la de vCard (la cual es OWL)
    \item bibliocyl.ttl - Datos de fuentes abiertas, previamente transformados a Turtle desde RDF/XML
    \item bibliocylapp.ttl - La app en Lyncex
\end{itemize}

La manera de cargarlas, mediante cURL, es la siguiente:
\begin{verbatim}
curl -X POST -H "Content-Type: text/turtle; charset=utf-8" --data-binary "@vcard.ttl" http://localhost:11011/_api
curl -X POST -H "Content-Type: text/turtle; charset=utf-8" --data-binary "@bibliocyl.ttl" http://localhost:11011/_api
curl -X POST -H "Content-Type: text/turtle; charset=utf-8" --data-binary "@bibliocylapp.ttl" http://localhost:11011/_api
\end{verbatim}

Una vez hecho esto, podemos acceder a diferentes páginas desde el navegador. La página de inicio estará disponible escribiendo localhost:11011 en el navegador:
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{tour/bibliocyl1.png}
    \caption{HomePage de BiblioCyL}
    \label{fig:bibliocyl1}
\end{figure}

Esto corresponde a las siguientes tripletas:
\begin{lstlisting}
<HomePage>
    a lcx:ContentController ;
    lcx:url "" ;
    lcx:method "get" ;
    lcx:content [
        a cnt:ContentAsText ;
        cnt:chars """
<h1>Bienvenido a BiblioCyL</h1>
<p>Aplicación creada por Adrián Arroyo</p>
<p><a href=/books>Ver listado bibliotecas</a></p>
"""
    ] ;
    lcx:mime "text/html" .
\end{lstlisting}

Hacemos click en el enlace. Esto nos lleva al BookListPage, el listado de todas las bibliotecas del dataset:

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{tour/bibliocyl2.png}
    \caption{BookListPage}
    \label{fig:bibliocyl2}
\end{figure}

Esto se corresponde a estas tripletas:
\begin{lstlisting}
<BookListPage>
    a lcx:TemplateController ;
    lcx:url "books" ;
    lcx:method "get" ;
    lcx:template [
        a cnt:ContentAsText ;
        cnt:chars """
        <h1>Listado de bibliotecas de Castilla y León</h1>
        <ul>
        {% each cards, card %}
            <li><a href=/book?id={{ card.id }}>{{ card.name }}</a></li>
        {% end %}
        </ul>
        """
    ] ;
    lcx:handler [
        a lcx:Handler ;
        lcx:handler_name "cards" ;
        lcx:code """
        handler(_Param, Output) :-
        findall(Card, (
            rdf(Biblio, rdf:type, 'http://www.w3.org/2006/vcard/ns#VCard'),
            db(Biblio, v:fn, Name),
            atom_concat('http://book.lyncex.com/aarroyoc/dev/lyncex/apps/', Id, Biblio),
            Card = _{id:Id, name:Name}
        ), Output).
        """
    ] .
\end{lstlisting}

Hacemos click en una biblioteca cualquiera, en este caso, la de Berlanga de Duero. Esto nos mostrará la vista con detalles de la biblioteca que son la URL y el correo, si existieran, mostrando además un mapa de su localización. Esto es el BookPage.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{tour/bibliocyl3.png}
    \caption{BookPage}
    \label{fig:bibliocyl3}
\end{figure}

El BookPage se implementa con estas tripletas:
\begin{lstlisting}
<BookPage>
    a lcx:TemplateController ;
    lcx:url "book" ;
    lcx:method "get" ;
    lcx:parameter [
        a lcx:Parameter ;
        lcx:param_name "id"
    ] ;
    lcx:template [
        a cnt:ContentAsText ;
        cnt:chars """
        <h1>{{ card.fn }}</h1>
        {% if exists(atom(url), card) %}
        <p>URL: <a href={{ card.url }}>{{ card.url }}</a></p>
        {% end %}
        {% if exists(atom(email), card) %}
        <p><a href={{ card.email }}>Email</a></p>
        {% end %}
        <iframe width=425 height=350 frameborder=0 scrolling=no marginheight=0 marginwidth=0 src=https://www.openstreetmap.org/export/embed.html?bbox=-10.206298828125002%2C39.232253141714914%2C1.3842773437500002%2C44.33956524809713&marker={{ geo.lat }}%2C{{ geo.lon }} style=border: 1px solid black></iframe>
        """
    ] ;
    lcx:query [
        a lcx:Query ;
        lcx:query_name "card" ;
        lcx:template_subject "http://book.lyncex.com/aarroyoc/dev/lyncex/apps/{{ id }}"
    ] ;
    lcx:handler [
        a lcx:Handler ;
        lcx:handler_name "geo" ;
        lcx:code """
        handler(Param, Output) :-
            get_dict(id, Param, Id),
            atom_concat('http://book.lyncex.com/aarroyoc/dev/lyncex/apps/', Id, Biblio),
            rdf(Biblio, v:geo, Geo),
            db(Geo, v:latitude, Lat),
            db(Geo, v:longitude, Lon),
            Output = _{lat:Lat, lon:Lon}.
        """
    ] .
\end{lstlisting}

Vamos a añadir una biblioteca nueva. Si tratamos de acceder a /admin/book tendremos un mensaje de \textit{Forbidden} y no podremos ver el formulario. Accedemos primero a /login, donde se nos mostrará el LoginPage:

\begin{figure}
    \centering
    \includegraphics{tour/bibliocyl4.png}
    \caption{LoginPage}
    \label{fig:bibliocyl4}
\end{figure}

\begin{lstlisting}
<LoginPage>
    a lcx:LoginController ;
    lcx:url "login" ;
    lcx:username "aarroyoc" ;
    lcx:password "123456" ;
    lcx:template  [
        a cnt:ContentAsText ;
        cnt:chars "{% unescape login %}"
    ] .
\end{lstlisting}

Una vez hayamos ingresado los credenciales correctos, ya podremos entrar a /admin/book (el BookForm). Se nos mostrará un formulario con diversos campos a rellenar. El primero es la IRI del sujeto sobre el que vamos a crear tripletas nuevas. Es necesario modificarlo, el valor que hay es solo una plantilla. Además para el campo geo deberemos indicar una IRI del sujeto geo. Este sujeto lo podemos crear entrando a /admin/geo (es el GeoForm).

\begin{figure}
    \centering
    \includegraphics{tour/bibliocyl5.png}
    \caption{BookForm}
    \label{fig:bibliocyl5}
\end{figure}

Si queremos editar el contenido podemos acceder a la URL /admin/book?_id=IRI, una vez allí veremos los campos rellenados, podremos editar el contenido y podremos borrar el sujeto del triplestore.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{tour/bibliocyl7.png}
    \caption{BookForm edición y borrado}
    \label{fig:bibliocyl7}
\end{figure}

\begin{lstlisting}
<BookForm>
    a lcx:FormController ;
    lcx:url "admin/book" ;
    lcx:access "private" ;
    lcx:base_subject "http://book.lyncex.com/aarroyoc/dev/lyncex/apps/" ;
    lcx:class <http://www.w3.org/2006/vcard/ns#VCard> ;
    lcx:template [
        a cnt:ContentAsText ;
        cnt:chars """
        <h1>Bibliotecas de Castilla y León</h1>
        <style>
        input{
            display:block;
            width:500px;
        }
        </style>
        {% unescape form %}
        """
    ] .
\end{lstlisting}

Con esto, hemos visto las características principales de Lyncex aplicadas sobre un conjunto de datos reales.


\chapter{Manuales}

En este capítulo se incluyen dos manuales para el uso de Lyncex.

\section{Manual de instalación}

El objetivo de este manual es disponer de un servidor de Lyncex funcionando correctamente en un equipo informático.

Lyncex es una aplicación implementada completamente en Prolog, sin embargo, no usa Prolog estándar sino que necesita específicamente el intérprete SWI Prolog. Este intérprete es open source y se encuentra disponible en gran cantidad de sistemas operativos y arquitecturas diferentes. Sin embargo, solo se ha probado su funcionamiento correcto en sistemas Linux con arquitectura x86 de 64 bits. Es necesario tener conexión a Internet durante la instalación.

\subsection{Docker}

Para disponer de un entorno lo más controlado y aislado posible del resto del sistema se usan tecnologías de contenedores, concretamente Docker. Una de las ventajas de Docker es que también se puede usar en MacOS y Windows, mediante una virtualización de Linux automática que realiza Docker.

El primer paso será tener instalado Docker y su utilidad Docker Compose. Este software suele estar en los repositorios de las principales distribuciones Linux. En Debian o Ubuntu sería así:
\begin{verbatim}
sudo apt update
sudo apt install docker.io docker-compose
\end{verbatim}

Una vez instalado podemos obtener el código de la aplicación. Para esto hará falta tener Git, aunque desde GitHub también se pueden generar comprimidos tar.gz o zip que no necesitan tener instalado Git:
\begin{verbatim}
git clone https://github.com/aarroyoc/lyncex/ lyncex
cd lyncex
\end{verbatim}

Una vez hemos descargado el código y tenemos Docker, podemos ejecutar el comando de Docker Compose para construir todas las imágenes de la aplicación:
\begin{verbatim}
docker-compose build
\end{verbatim}

Esto comenzará la descarga de Prolog, así como de Python y numerosas dependencias. Guardará las imágenes en el almacén local de Docker. 

Ahora podemos realizar la configuración de los contenedores. Edita el fichero docker-compose.yml con un editor de textos cualquiera. El campo más importante es el de ports. Aquí podemos configurar el puerto sobre el que va a estar escuchando Lyncex. Para realizar el cambio hay que modificar "11011:11011" a "PUERTO\_DESEADO:11011". El resto de configuraciones no son de uso común.

Una vez configurado, ya podemos ejecutar Lyncex:
\begin{verbatim}
docker-compose run --service-ports lyncex
\end{verbatim}
Y Lyncex estará ya disponible.

\subsection{Tests}

Una vez instalado puede ser interesante ejecutar los tests automatizados para comprobar que efectivamente la versión instalada tiene un cierto nivel de confiabilidad.

Para ello lo primero es suspender la ejecución del contenedor web, con Control-C o mediante el comando siguiente:
\begin{verbatim}
docker-compose down
\end{verbatim}

Para ejecutar los tests unitarios:
\begin{verbatim}
docker-compose run test-unit
\end{verbatim}

Para ejecutar los tests E2E:
\begin{verbatim}
docker-compose run test-e2e
\end{verbatim}

Una vez finalizada la ejecución de los tests, hay que destruir todos los contenedores. Para ello volvemos a ejecutar:
\begin{verbatim}
docker-compose down
\end{verbatim}

\section{Manual de usuario}

En este manual veremos como usar Lyncex una vez instalado. 

\subsection{API HTTP}
Para el manual usaremos la herramienta cURL, para realizar peticiones HTTP mediante comandos sencillos. También se pueden usar otras herramientas similares como Postman o incluso, realizar las llamadas HTTP desde un programa personalizado.

Vamos a supone que Lyncex se encuentra ejecutándose en localhost en el puerto 11011.

La primera tarea que debe realizarse es subir tripletas. Estas deben estar guardadas en un fichero Turtle. Para subir tripletas usaremos el verbo POST de HTTP y el endpoint de la API es /\_api.

Veamos un ejemplo, si tenemos este fichero Turtle, con el nombre author.ttl:

\begin{lstlisting}
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix dc: <http://purl.org/dc/elements/1.1/> .
@prefix ex: <http://example.org/stuff/1.0/> .

<http://www.w3.org/TR/rdf-syntax-grammar>
    dc:title "RDF/XML Syntax Specification (Revised)" ;
    ex:editor [
        ex:fullname "Dave Beckett";
        ex:homePage <http://purl.org/net/dajobe/>
    ] .
\end{lstlisting}

Podemos guardar las tripletas de la siguiente forma:
\begin{verbatim}
curl -X POST -H "Content-Type: text/turtle; charset=utf-8" --data-binary "@author.ttl" http://localhost:11011/_api
\end{verbatim}

Si el fichero es correcto y la validación de ontologías también lo es, responderá OK.

La siguiente operación que podemos hacer es obtener de vuelta los datos insertados en el triplestore. Para ello usamos el verbo GET y el endpoint /\_api/query.

\begin{verbatim}
curl -X GET http://localhost:11011/_api/query
\end{verbatim}

El resultado seguramente sea diferente al fichero original, pero equivalente en cuanto a información:
\begin{verbatim}                                                 
@prefix    dc: <http://purl.org/dc/elements/1.1/> .
@prefix stuff: <http://example.org/stuff/1.0/> .

# Named toplevel resources (1)

<http://www.w3.org/TR/rdf-syntax-grammar>
    stuff:editor [ stuff:fullname "Dave Beckett" ;
                stuff:homePage <http://purl.org/net/dajobe/>
                ] ;
    dc:title "RDF/XML Syntax Specification (Revised)" .
\end{verbatim}

La API GET soporta filtros, para obtener grafos parciales. Para ello se fija el valor del sujeto, objeto o predicado a un valor fijo y se sacan todas las tripletas que cumplen con ese valor fijo en la posición correspondiente. Esto se hace mediante los parámetros GET llamados subject, predicate y object respectivamente. Por ejemplo, si queremos filtrar por las tripletas que tienen dc:title como predicado:

\begin{verbatim}
curl -X GET "http://localhost:11011/_api/query?predicate=http://purl.org/dc/elements/1.1/title"
\end{verbatim}

Obtendremos como resultado:
\begin{verbatim}
@prefix dc: <http://purl.org/dc/elements/1.1/> .

# Named toplevel resources (1)

<http://www.w3.org/TR/rdf-syntax-grammar>
    dc:title "RDF/XML Syntax Specification (Revised)" .
\end{verbatim}

También podemos borrar las tripletas almacenadas mediante el verbo DELETE y el endpoint /\_api/delete. Este endpoint también soporta parámetros similar al GET con filtros. El uso es sencillo:

\begin{verbatim}
curl -X DELETE "http://localhost:11011/_api/delete"
\end{verbatim}

\subsection{Uso de la ontología}

USO BASICO
MULTIPLICIDAD
VARIABLES MAGICAS
PREDICADOS EXTRA (db, exists)

\chapter{Conclusiones y trabajo futuro}

MUTABILIDAD RDF
HERRAMIENTAS PROLOG
BDD
ANEXO TESTS

TRIPLESTORE DESACOPLADO
SEGURIDAD (CONTROL DE ACCESO FINO, gestion de secretos)


\begin{appendices}
\chapter{Ontología}
\lstinputlisting{lyncex.ttl}
\end{appendices}
\bibliographystyle{acm}

\bibliography{ref}

\end{document}


% Los objetivos principales que se propusieron para Lyncex y que debían ser cumplidas son:

% \begin{itemize}
%     \item US1: Almacenar tripletas de datos junto a sus ontologías en el mismo espacio, validándose
%     \item US2: Desarrollar una ontología que permita definir aplicaciones web sobre datos almacenados en el propio almacenamiento
%     \item US3: Servir páginas web estáticas
%     \item US4: Servir páginas web que necesiten leer datos del almacenamiento (plantillas)
%     \item US5: Servir páginas web con formularios
% \end{itemize}

% Otros objetivos que estaría bien cumplir, pero no son necesarios para tener una versión funcional son:
% \begin{itemize}
%     \item OUS1: Servir páginas web con sistemas de autenticación y autorización propios
%     \item OUS2: Servir APIs web de forma similar al contenido HTML
%     \item OUS3: Tener un endpoint de consulta SPARQL
% \end{itemize}